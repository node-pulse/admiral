# Node Pulse Metrics Architecture - Complete Documentation

**Last Updated:** 2025-10-31
**Status:** Production Ready
**Version:** 2.0 (Simplified Metrics + Process Monitoring)

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Simplified Metrics Architecture](#simplified-metrics-architecture)
3. [Process Monitoring Feature](#process-monitoring-feature)
4. [Complete Data Flow](#complete-data-flow)
5. [Database Schema](#database-schema)
6. [API Endpoints](#api-endpoints)
7. [Frontend Components](#frontend-components)
8. [Deployment Guide](#deployment-guide)
9. [Performance & Benefits](#performance--benefits)
10. [Files Reference](#files-reference)

---

## Executive Summary

Node Pulse Dashboard uses a **simplified metrics architecture** that achieves:
- âœ… **98.32% bandwidth reduction** (61KB â†’ 1KB per scrape)
- âœ… **99.8% database reduction** (1100+ rows â†’ 1 row per scrape)
- âœ… **10-30x faster queries** (direct column access vs JSONB parsing)
- âœ… **Process monitoring** (Top 10 processes by CPU/Memory)

### Architecture Philosophy

**Agent-side parsing** offloads work from central server:
```
node_exporter (localhost:9100) â†’ Agent parses locally â†’ Sends 39 metrics (1KB JSON) â†’ Submarines â†’ PostgreSQL
process_exporter (localhost:9256) â†’ Agent parses locally â†’ Sends N processes â†’ Submarines â†’ PostgreSQL
```

**Benefits:**
- Distributed parsing load across agents
- Minimal network usage
- Fast database inserts
- Simple, clean code

---

## Simplified Metrics Architecture

### Overview

Replaced generic Prometheus `metric_samples` table (JSONB-based) with a simplified `metrics` table storing only 39 essential metrics in dedicated columns.

### Agent Flow

```
Agent â†’ Scrapes node_exporter (localhost:9100, Prometheus text format)
      â†’ Parses locally using agent/internal/prometheus/parser.go
      â†’ Extracts 39 essential metrics into MetricSnapshot struct
      â†’ Sends compact JSON (~1KB) to Submarines
      â†’ Submarines deserializes and inserts 1 row into PostgreSQL
```

### Database Schema

```sql
CREATE TABLE admiral.metrics (
    id BIGSERIAL PRIMARY KEY,
    server_id TEXT NOT NULL,
    timestamp TIMESTAMP WITH TIME ZONE NOT NULL,

    -- CPU (6 fields) - raw counter values
    cpu_idle_seconds DOUBLE PRECISION,
    cpu_iowait_seconds DOUBLE PRECISION,
    cpu_system_seconds DOUBLE PRECISION,
    cpu_user_seconds DOUBLE PRECISION,
    cpu_steal_seconds DOUBLE PRECISION,
    cpu_cores INTEGER,

    -- Memory (7 fields) - raw bytes
    memory_total_bytes BIGINT,
    memory_available_bytes BIGINT,
    memory_free_bytes BIGINT,
    memory_cached_bytes BIGINT,
    memory_buffers_bytes BIGINT,
    memory_active_bytes BIGINT,
    memory_inactive_bytes BIGINT,

    -- Swap (3 fields) - raw bytes
    swap_total_bytes BIGINT,
    swap_free_bytes BIGINT,
    swap_cached_bytes BIGINT,

    -- Disk (3 fields) - raw bytes for root filesystem
    disk_total_bytes BIGINT,
    disk_free_bytes BIGINT,
    disk_available_bytes BIGINT,

    -- Disk I/O (5 fields) - counters
    disk_reads_completed_total BIGINT,
    disk_writes_completed_total BIGINT,
    disk_read_bytes_total BIGINT,
    disk_written_bytes_total BIGINT,
    disk_io_time_seconds_total DOUBLE PRECISION,

    -- Network (8 fields) - counters for primary interface
    network_receive_bytes_total BIGINT,
    network_transmit_bytes_total BIGINT,
    network_receive_packets_total BIGINT,
    network_transmit_packets_total BIGINT,
    network_receive_errs_total BIGINT,
    network_transmit_errs_total BIGINT,
    network_receive_drop_total BIGINT,
    network_transmit_drop_total BIGINT,

    -- System Load (3 fields)
    load_1min DOUBLE PRECISION,
    load_5min DOUBLE PRECISION,
    load_15min DOUBLE PRECISION,

    -- Processes (3 fields)
    processes_running INTEGER,
    processes_blocked INTEGER,
    processes_total INTEGER,

    -- Uptime (1 field)
    uptime_seconds BIGINT,

    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Optimized indexes
CREATE INDEX idx_metrics_lookup ON admiral.metrics(server_id, timestamp DESC);
CREATE INDEX idx_metrics_timestamp ON admiral.metrics(timestamp DESC);
CREATE INDEX idx_metrics_server_created ON admiral.metrics(server_id, created_at DESC);
```

**Total**: 39 metric fields + metadata = **1 row per scrape** (instead of 1100+)

### Network Usage Comparison

#### Before (Raw Prometheus)
```
Scrape size: 61,698 bytes (1100+ lines of Prometheus text)
Frequency: 15 seconds (4 per minute)
Per month: 8.6GB per server
```

#### After (Parsed JSON)
```json
{
  "timestamp": "2025-10-30T19:29:49Z",
  "cpu_idle_seconds": 7178001.5,
  "cpu_iowait_seconds": 295.19,
  "cpu_system_seconds": 2979.08,
  "cpu_user_seconds": 7293.29,
  "cpu_steal_seconds": 260.7,
  "cpu_cores": 4,
  "memory_total_bytes": 8326443008,
  "memory_available_bytes": 7920050176,
  ...
  (39 total fields)
}

Scrape size: 1,034 bytes
Frequency: 15 seconds
Per month: 120MB per server

SAVINGS: 98.6% reduction (8.6GB â†’ 120MB per server per month)
```

### Key Design Decisions

#### 1. Raw Values Instead of Percentages
**Why?**
- More flexible - dashboard can calculate any metric
- Better for alerting - can alert on absolute values
- Enables rate calculations - counter deltas give accurate throughput
- Future-proof - new calculations without schema changes

**How Percentages are Calculated**:
- CPU: `100 - (idle_delta / total_delta * 100)`
- Memory: `(total - available) / total * 100`
- Disk: `(total - available) / total * 100`

#### 2. No Foreign Keys
**Why?**
- Better performance (no FK lookup overhead)
- More flexibility (can keep metrics after server deletion)
- Simpler migrations (no cascade issues)
- Faster bulk inserts

#### 3. Agent-Side Parsing
**Why?**
- Offloads work from central server
- Distributes parsing load across agents
- Reduces network usage by 98%
- Submarines only deserializes JSON (minimal CPU)

#### 4. Primary Interface/Disk Selection
**Network Priority**: eth0 > en0 > ens3 > first available (excluding lo)
**Disk Priority**: vda > sda > nvme0n1 > first available

---

## Process Monitoring Feature

### Overview

The **Top 10 Processes by CPU/Memory** feature provides per-process monitoring using `process_exporter`.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Target Server                                                  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  node_exporter   â”‚  â”‚ process_exporter â”‚                    â”‚
â”‚  â”‚  :9100/metrics   â”‚  â”‚  :9256/metrics   â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚           â”‚                     â”‚                                â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                      â”‚ Scrape                                    â”‚
â”‚                      â–¼                                           â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚           â”‚  NodePulse Agent   â”‚                                 â”‚
â”‚           â”‚  - Multi-exporter  â”‚                                 â”‚
â”‚           â”‚  - Buffer & Push   â”‚                                 â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ HTTPS POST
                      â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Submarines Ingest   â”‚
           â”‚  /metrics/prometheus â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              Valkey Stream â†’ Digest â†’ PostgreSQL
```

### Agent Payload Format

The agent sends a **flat array** structure for process_exporter (no wrappers):

```json
{
  "node_exporter": [
    {
      "timestamp": "2025-10-30T12:00:00Z",
      "cpu_idle_seconds": 7184190.53,
      ... (39 fields)
    }
  ],
  "process_exporter": [
    {
      "timestamp": "2025-10-30T12:00:00Z",
      "name": "nginx",
      "num_procs": 4,
      "cpu_seconds_total": 1234.56,
      "memory_bytes": 104857600
    },
    {
      "timestamp": "2025-10-30T12:00:00Z",
      "name": "postgres",
      "num_procs": 8,
      "cpu_seconds_total": 5678.90,
      "memory_bytes": 512000000
    },
    {
      "timestamp": "2025-10-30T12:00:00Z",
      "name": "systemd",
      "num_procs": 1,
      "cpu_seconds_total": 45.23,
      "memory_bytes": 15728640
    }
  ]
}
```

### Database Schema (process_snapshots)

```sql
CREATE TABLE admiral.process_snapshots (
    id BIGSERIAL PRIMARY KEY,
    server_id TEXT NOT NULL,
    timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
    process_name TEXT NOT NULL,           -- Command name (e.g., "nginx", "postgres")
    num_procs INTEGER NOT NULL,            -- Number of processes in this group
    cpu_seconds_total DOUBLE PRECISION,    -- Total CPU time consumed (counter)
    memory_bytes BIGINT,                   -- Resident memory (RSS) in bytes
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Indexes for efficient querying
CREATE INDEX idx_process_snapshots_lookup
    ON admiral.process_snapshots(server_id, timestamp DESC);
CREATE INDEX idx_process_snapshots_server_name
    ON admiral.process_snapshots(server_id, process_name, timestamp DESC);
CREATE INDEX idx_process_snapshots_server_time
    ON admiral.process_snapshots(server_id, timestamp DESC, cpu_seconds_total DESC, memory_bytes DESC);
```

### Data Flow

**Simple & Clean Pattern:**
1. **Agent**: Sends `[{name: "nginx", ...}, {name: "postgres", ...}]` (flat array)
2. **Ingest**: Publishes each process individually (N messages to Valkey Stream)
3. **Digest**: Processes each message â†’ Inserts 1 row
4. **Database**: N rows (one per process group)

**Pattern Consistency**:
- `node_exporter`: 1 snapshot â†’ 1 Valkey message â†’ 1 DB row
- `process_exporter`: N snapshots â†’ N Valkey messages â†’ N DB rows
- **Both use the same simple pattern** - no special wrapping needed!

### Key Design

- **Agent sends flat array**: Each process becomes one `ProcessExporterMetricSnapshot` in the array
- **Ingest handler**: Publishes each process snapshot individually to Valkey Stream
- **Digest worker**: Processes each snapshot and inserts one row per process
- **Database storage**: Each snapshot â†’ one row in `process_snapshots` table
- **CPU calculation**: Uses counter deltas with LAG window function
- **Memory**: Instantaneous RSS value
- **Retention**: 7 days for process snapshots

---

## Complete Data Flow

### End-to-End Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Linux Server                                                 â”‚
â”‚    - node_exporter on localhost:9100                            â”‚
â”‚    - process_exporter on localhost:9256                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Prometheus text format
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. NodePulse Agent                                              â”‚
â”‚    - Scrapes both exporters locally                             â”‚
â”‚    - Parses Prometheus text format                              â”‚
â”‚    - Extracts essential metrics                                 â”‚
â”‚    - Builds multi-exporter JSON payload                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTPS POST /metrics/prometheus
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Submarines Ingest (Go)                                       â”‚
â”‚    - Receives multi-exporter JSON                               â”‚
â”‚    - Parses using json.RawMessage (flexible)                    â”‚
â”‚    - node_exporter: 1 message to Valkey                         â”‚
â”‚    - process_exporter: N messages to Valkey                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Valkey Stream: nodepulse:metrics:stream
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Valkey Stream                                                â”‚
â”‚    - Buffers messages (backpressure handling)                   â”‚
â”‚    - Consumer group: submarines-digest                          â”‚
â”‚    - Reads in batches of 10                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ XREADGROUP
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Submarines Digest (Go)                                       â”‚
â”‚    - Consumes from stream                                       â”‚
â”‚    - Routes by exporter_name:                                   â”‚
â”‚      * node_exporter â†’ metrics table                            â”‚
â”‚      * process_exporter â†’ process_snapshots table               â”‚
â”‚    - Updates server last_seen_at                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ INSERT statements
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. PostgreSQL                                                   â”‚
â”‚    - admiral.metrics (39 columns, 1 row per scrape)             â”‚
â”‚    - admiral.process_snapshots (N rows per scrape)              â”‚
â”‚    - Optimized indexes for fast queries                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Eloquent ORM queries
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Laravel Backend                                              â”‚
â”‚    - DashboardController: LAG() for CPU rates                   â”‚
â”‚    - ProcessController: Top 10 queries                          â”‚
â”‚    - Direct column access (no JSONB)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ JSON API responses
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. React Frontend (Inertia.js)                                 â”‚
â”‚    - MetricsChart: Time-series charts                           â”‚
â”‚    - ProcessList: Top 10 processes table                        â”‚
â”‚    - Real-time updates                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  ðŸ‘¤ User sees metrics!
```

### Field Mapping (End-to-End)

#### Node Exporter Metrics
| Layer | Field Example | Type | Notes |
|-------|---------------|------|-------|
| **node_exporter** | `node_cpu_seconds_total{mode="idle"}` | Counter | Prometheus metric |
| **Agent Parser** | `CPUIdleSeconds` | float64 | Aggregated across CPUs |
| **Agent JSON** | `cpu_idle_seconds` | number | JSON payload |
| **Ingest** | `snapshot.CPUIdleSeconds` | float64 | Go struct |
| **PostgreSQL** | `cpu_idle_seconds` | DOUBLE PRECISION | Column |
| **Laravel** | `cpu_idle_seconds` | float | Eloquent |
| **API** | `cpu_usage_percent` | number | **Calculated** with LAG |
| **React** | Percentage chart | visual | Chart.js |

#### Process Exporter Metrics
| Layer | Field Example | Type | Notes |
|-------|---------------|------|-------|
| **process_exporter** | `namedprocess_namegroup_cpu_seconds_total{groupname="nginx"}` | Counter | Prometheus |
| **Agent Parser** | `CPUSecondsTotal` | float64 | Per process |
| **Agent JSON** | `cpu_seconds_total` | number | Flat array |
| **Ingest** | `snapshot.CPUSecondsTotal` | float64 | Individual |
| **PostgreSQL** | `cpu_seconds_total` | DOUBLE PRECISION | One row |
| **Laravel** | `cpu_seconds_total` | float | Eloquent |
| **API** | `avg_cpu_percent` | number | LAG calculation |
| **React** | Table cell | text | Formatted % |

---

## Database Schema

### Tables Overview

```sql
-- Main metrics table (39 essential system metrics)
admiral.metrics
  - 1 row per scrape
  - Raw counter values
  - Direct column access

-- Process monitoring table
admiral.process_snapshots
  - N rows per scrape (one per process)
  - Process-level metrics
  - CPU/Memory per process

-- Server registry
admiral.servers
  - Server metadata
  - Connection info
  - Last seen timestamp

-- Alert configuration
admiral.alert_rules
  - Alert definitions
  - Thresholds
  - Notifications

-- Alert history
admiral.alerts
  - Triggered alerts
  - Status tracking
```

### Retention Policies

Implemented via `submarines/internal/cleaner/`:

| Table | Retention | Cleanup Frequency |
|-------|-----------|-------------------|
| `metrics` | 30 days | Every 1 hour |
| `process_snapshots` | 7 days | Every 1 hour |
| `alerts` | 90 days | Daily |

### Index Strategy

**For time-series queries:**
- Composite indexes with `(server_id, timestamp DESC)`
- Enables efficient range scans
- Supports ORDER BY optimization

**For top-N queries:**
- Multi-column indexes with sort columns
- Example: `(server_id, timestamp DESC, cpu_seconds_total DESC)`

---

## API Endpoints

### Metrics Ingestion

#### POST /metrics/prometheus
**Purpose:** Primary endpoint for simplified metrics (parsed JSON, multi-exporter)

**Request:**
```json
{
  "node_exporter": [
    { "timestamp": "...", "cpu_idle_seconds": ..., ... }
  ],
  "process_exporter": [
    { "timestamp": "...", "name": "nginx", ... }
  ]
}
```

**Query Parameters:**
- `server_id` (required): Server UUID

**Response:**
```json
{
  "status": "success",
  "snapshots": 11,
  "server_id": "uuid",
  "first_message_id": "1698765432100-0"
}
```

### Dashboard APIs

#### GET /api/dashboard/stats
**Purpose:** Overview statistics

**Response:**
```json
{
  "total_servers": 25,
  "online_servers": 23,
  "offline_servers": 2,
  "active_alerts": 3
}
```

#### GET /api/servers/{id}/metrics
**Purpose:** Get server metrics for charts

**Query Parameters:**
- `hours`: Time range (default: 24)
- `metric`: Specific metric type

**Response:**
```json
{
  "server_id": "uuid",
  "metrics": [
    {
      "timestamp": "2025-10-31T12:00:00Z",
      "cpu_usage_percent": 45.2,
      "memory_usage_percent": 62.1,
      ...
    }
  ]
}
```

### Process Monitoring APIs

#### GET /api/processes/top
**Purpose:** Get top N processes by CPU or memory

**Query Parameters:**
- `server_ids[]`: Array of server UUIDs (required)
- `metric`: 'cpu' | 'memory' (default: 'cpu')
- `limit`: 1-50 (default: 10)
- `hours`: 1-168 (default: 1)

**Response:**
```json
{
  "metric": "cpu",
  "time_range_hours": 1,
  "processes": [
    {
      "name": "postgres",
      "avg_cpu_percent": 45.23,
      "avg_memory_mb": 488.32,
      "peak_memory_mb": 512.00,
      "avg_num_procs": 8
    }
  ]
}
```

**SQL Implementation:**
```sql
WITH deltas AS (
    SELECT
        process_name,
        cpu_seconds_total,
        LAG(cpu_seconds_total) OVER (
            PARTITION BY server_id, process_name
            ORDER BY timestamp
        ) as prev_cpu,
        LAG(timestamp) OVER (
            PARTITION BY server_id, process_name
            ORDER BY timestamp
        ) as prev_ts,
        memory_bytes,
        num_procs
    FROM admiral.process_snapshots
    WHERE server_id IN (...)
        AND timestamp >= NOW() - INTERVAL '1 hour'
)
SELECT
    process_name,
    ROUND(AVG(
        (cpu_seconds_total - prev_cpu) /
        EXTRACT(EPOCH FROM (timestamp - prev_ts)) * 100
    )::numeric, 2) as avg_cpu_percent,
    ROUND(AVG(memory_bytes / 1024.0 / 1024.0)::numeric, 2) as avg_memory_mb,
    ROUND(MAX(memory_bytes / 1024.0 / 1024.0)::numeric, 2) as peak_memory_mb,
    ROUND(AVG(num_procs)::numeric, 0) as avg_num_procs
FROM deltas
WHERE prev_cpu IS NOT NULL
GROUP BY process_name
ORDER BY avg_cpu_percent DESC
LIMIT 10;
```

---

## Frontend Components

### Technology Stack

- **Framework**: React 19 + TypeScript
- **SSR**: Inertia.js (Laravel integration)
- **UI Library**: shadcn/ui (Radix UI + Tailwind CSS)
- **Charts**: Recharts
- **State**: React hooks (useState, useEffect)

### Key Components

#### MetricsChart (`components/servers/metrics-chart.tsx`)
**Purpose:** Time-series charts for system metrics

**Features:**
- Multiple metric types (CPU, Memory, Disk, Network)
- Time range selector
- Real-time updates
- Responsive design
- Tooltips with formatted values

**Usage:**
```tsx
<MetricsChart
  selectedServers={['uuid1', 'uuid2']}
  metricType="cpu"
  timeRange={24}
/>
```

#### ProcessList (`components/servers/process-list.tsx`)
**Purpose:** Top 10 processes table

**Features:**
- CPU/Memory toggle tabs
- Time range selector (1h, 6h, 24h, 7d)
- Auto-refresh on selection change
- Loading and empty states
- Formatted values (MB/GB, percentages)

**Usage:**
```tsx
<ProcessList selectedServers={['uuid1', 'uuid2']} />
```

**UI Elements:**
- Tabs (shadcn/ui) - CPU/Memory toggle
- Select (shadcn/ui) - Time range
- Table (shadcn/ui) - Data display
- Card (shadcn/ui) - Container

#### ServerSelector (`components/servers/server-selector.tsx`)
**Purpose:** Multi-select server picker

**Features:**
- Checkbox selection
- Online/offline status indicators
- Search/filter
- Select all/none

### Dashboard Layout

```tsx
<Dashboard>
  <DashboardStats />         {/* Total/Online/Offline servers */}
  <ServerSelector />         {/* Select servers */}
  <MetricsChart />          {/* System metrics charts */}
  <ProcessList />           {/* Top 10 processes */}
</Dashboard>
```

---

## Deployment Guide

### Prerequisites

- PostgreSQL 18
- Valkey (Redis-compatible)
- Go 1.24+ (for Submarines)
- PHP 8.2+ (for Flagship)
- Node.js 20+ (for frontend build)

### Step 1: Database Migration

```bash
cd migrate
./migrate up
```

**Creates:**
- `admiral.metrics` table
- `admiral.process_snapshots` table
- All indexes

### Step 2: Deploy Submarines

```bash
cd submarines
go build ./cmd/ingest
go build ./cmd/digest

# Start ingest (HTTP server)
./ingest

# Start digest (background worker)
./digest
```

**Or use Docker:**
```bash
docker compose up -d submarines-ingest submarines-digest
```

### Step 3: Deploy process_exporter to Servers

```bash
cd ansible
ansible-playbook playbooks/prometheus/deploy-process-exporter.yml -i inventory.yml
```

**Verifies:**
- process_exporter installed at `/usr/local/bin/process-exporter`
- Service running on localhost:9256
- Metrics endpoint accessible

### Step 4: Configure NodePulse Agent

Edit `/etc/nodepulse/nodepulse.yml`:

```yaml
scrapers:
  prometheus:
    enabled: true
    endpoints:
      # Existing node_exporter
      - url: "http://127.0.0.1:9100/metrics"
        name: "node_exporter"
        interval: 15s

      # Add process_exporter
      - url: "http://127.0.0.1:9256/metrics"
        name: "process_exporter"
        interval: 15s

server:
  endpoint: "https://dashboard.example.com/metrics/prometheus"
  format: "prometheus"
  timeout: 10s

agent:
  server_id: "auto-generated-uuid"
  interval: 15s

buffer:
  enabled: true
  retention_hours: 48
  max_size_mb: 100
```

Restart agent:
```bash
sudo systemctl restart nodepulse
```

### Step 5: Deploy Flagship (Laravel)

```bash
cd flagship
composer install --no-dev
npm install
npm run build

php artisan migrate
php artisan config:cache
php artisan route:cache
php artisan view:cache
```

**Web server (Caddy/Nginx):**
```
Reverse proxy to:
- :8080 â†’ Submarines Ingest
- :8000 â†’ Flagship (Laravel)
```

### Step 6: Verify Data Flow

```bash
# Check agent logs
sudo journalctl -u nodepulse -f | grep process_exporter

# Check database
psql -U admiral -d admiral -c "SELECT COUNT(*) FROM admiral.process_snapshots;"

# Check API
curl "http://localhost/api/processes/top?server_ids[]=<uuid>&metric=cpu&hours=1" | jq
```

### Step 7: Access Dashboard

1. Open `https://dashboard.example.com`
2. Login with credentials
3. Navigate to Dashboard
4. Select servers
5. View metrics and top processes

---

## Performance & Benefits

### Network Bandwidth

| Metric | Before | After | Savings |
|--------|--------|-------|---------|
| Per scrape | 61KB | 1KB | 98.32% |
| Per day (1 server) | 288MB | 4MB | 98.6% |
| Per month (100 servers) | 860GB | 12GB | 98.6% |

### Database Storage

| Metric | Before | After | Savings |
|--------|--------|-------|---------|
| Rows per scrape | 1100+ | 1 | 99.8% |
| Disk space/scrape | ~220KB | ~300B | 99.8% |
| Query complexity | JSONB parsing | Direct columns | N/A |

### Query Performance

| Query Type | Before | After | Improvement |
|------------|--------|-------|-------------|
| Latest metrics | 150ms | 5-15ms | 10-30x faster |
| Time-series | 800ms | 50-80ms | 10-16x faster |
| JSONB parsing | Required | None | Eliminated |

### Process Monitoring Performance

| Metric | Value |
|--------|-------|
| Rows per server/hour | ~14,400 (100 processes Ã— 240 scrapes) |
| Storage per server/hour | ~1.4 MB |
| 7-day retention | ~235 MB per server |
| Top 10 query time | <50ms (with indexes) |
| API response time | 30-80ms |

### Architecture Benefits

âœ… **Simplicity**
- No unnecessary wrapper structs
- Straightforward data flow
- Easy to understand and debug

âœ… **Consistency**
- Same pattern for all exporters
- Predictable behavior
- Easier maintenance

âœ… **Scalability**
- Each process is independent
- Valkey Stream handles backpressure
- Can process in parallel
- Digest workers can scale horizontally

âœ… **Flexibility**
- Easy to add new exporters
- No rigid structure constraints
- Future-proof design
- Raw values enable flexible calculations

---

## Files Reference

### Agent Repository (`../agent/`)

**Core:**
- `internal/prometheus/node_exporter_parser.go` - Parses node_exporter metrics
- `internal/prometheus/process_exporter_parser.go` - Parses process_exporter metrics
- `internal/exporters/node_exporter.go` - node_exporter client
- `internal/exporters/process_exporter.go` - process_exporter client
- `internal/report/sender.go` - Multi-exporter payload builder
- `internal/report/buffer.go` - Write-ahead log for reliability

**Configuration:**
- `internal/config/config.go` - Multi-exporter configuration
- `nodepulse.yml` - Example configuration

### Admiral Repository

**Submarines (Go):**
- `submarines/internal/handlers/prometheus.go` - Ingest handler
- `submarines/cmd/digest/main.go` - Digest worker
- `submarines/cmd/ingest/main.go` - HTTP server
- `submarines/internal/database/database.go` - PostgreSQL client
- `submarines/internal/valkey/valkey.go` - Valkey Streams client
- `submarines/internal/cleaner/metrics.go` - Retention cleanup

**Database:**
- `migrate/migrations/20251016211918470_initial_schema.sql` - metrics table
- `migrate/migrations/20251030203553001_create_process_snapshots_table.sql` - process_snapshots table

**Flagship (Laravel):**
- `flagship/app/Models/Metric.php` - Eloquent model for metrics
- `flagship/app/Models/ProcessSnapshot.php` - Eloquent model for processes
- `flagship/app/Models/Server.php` - Server model with helpers
- `flagship/app/Http/Controllers/DashboardController.php` - Dashboard API
- `flagship/app/Http/Controllers/ProcessController.php` - Process API
- `flagship/routes/api.php` - API routes

**Frontend (React):**
- `flagship/resources/js/Pages/dashboard.tsx` - Main dashboard
- `flagship/resources/js/components/servers/metrics-chart.tsx` - Metrics charts
- `flagship/resources/js/components/servers/process-list.tsx` - Process table
- `flagship/resources/js/components/servers/server-selector.tsx` - Server picker

**Ansible:**
- `ansible/roles/node-exporter/` - node_exporter deployment
- `ansible/roles/process-exporter/` - process_exporter deployment
- `ansible/playbooks/prometheus/deploy-node-exporter.yml` - Playbook
- `ansible/playbooks/prometheus/deploy-process-exporter.yml` - Playbook

**Documentation:**
- `docs/metrics-architecture.md` - **THIS FILE** - Complete documentation
- `CLAUDE.md` - Project overview for AI assistants

---

## Success Criteria âœ…

### Simplified Metrics
- âœ… Agent parses node_exporter correctly
- âœ… Sends 39 essential metrics in ~1KB payload
- âœ… 98.32% bandwidth reduction achieved
- âœ… Database stores 1 row per scrape
- âœ… Queries 10-30x faster
- âœ… Dashboard displays real-time charts
- âœ… LAG() window functions for accurate CPU%

### Process Monitoring
- âœ… Agent parses process_exporter correctly
- âœ… Sends flat array (no wrappers)
- âœ… Submarines routes correctly
- âœ… Database stores per-process data
- âœ… API returns accurate top 10
- âœ… Frontend displays interactive table
- âœ… CPU/Memory toggle works
- âœ… Time range selector works
- âœ… Ansible deployment automated

### Overall System
- âœ… No payload structure mismatches
- âœ… Clean, maintainable code
- âœ… Complete documentation
- âœ… Production-ready
- âœ… Scalable architecture

---

## Revision History

| Date | Version | Changes |
|------|---------|---------|
| 2025-10-30 | 1.0 | Initial simplified metrics implementation |
| 2025-10-31 | 2.0 | Added process monitoring feature, fixed payload structure |

---

**Status:** âœ… Production Ready
**Last Reviewed:** 2025-10-31
**Next Review:** When adding new exporter types
